{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mahdiye-Bayat/firstDive/blob/master/CRESA/RNN_Surv.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/gdrive')\n",
        "root_path = 'gdrive/My Drive/Data_Prep'\n",
        "os.chdir(root_path)"
      ],
      "metadata": {
        "id": "wtivg_8PP4-w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d8a40c3-0600-42cf-f2cb-13c035ee1523"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_XDEOieUFIh_"
      },
      "outputs": [],
      "source": [
        "from pandas.core.arrays import boolean\n",
        "#data_handler\n",
        "#%% \n",
        "import pandas as pd\n",
        "import scipy.io as sio\n",
        "import numpy as np\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "\n",
        "#%% \n",
        "\n",
        "\"\"\"\n",
        "Input: string (dataset name)\n",
        "Output: imported data\n",
        "\"\"\"\n",
        "\n",
        "def import_small_dataset(dataset): \n",
        " \n",
        "    path = ''\n",
        "    path += str(dataset)\n",
        "    path += '.csv'\n",
        "    \n",
        "    data = pd.read_csv(path)\n",
        "        \n",
        "    data = data.drop(data.columns[0], axis = 1)\n",
        "    \n",
        "    if(dataset == 'nwtco'):\n",
        "        data['in.subcohort'] = data['in.subcohort'].astype(float)\n",
        "    if(dataset == 'myData'):\n",
        "        data['event'] = data['event'].astype(bool).astype(float)\n",
        "    \n",
        "    return data\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Input: string (dataset name)\n",
        "Output: imported data\n",
        "\"\"\"\n",
        "\n",
        "def import_UNOS_dataset(dataset): \n",
        "    if (dataset == 'Transplant'):\n",
        "        data = sio.loadmat('UNOS_Data_Share/Preprocessed_Data/Transplant_Data')\n",
        "        #Assign more meaningful names to features\n",
        "        data ['Feature_Name'] = ['Age', 'Gender', 'IschemicTime','Diabetes', 'Infection', 'Transfusion','PreviousTransplant','NumberOfPreviousTransplant','VentilatorAssist','ECMOAssist','VentSupport', 'Creatinine','Bilirubin', 'PRA','HLAMismatch','BloodTypeA','BloodTypeB','BloodTypeO','BloodTypeAB','Dialysis','IABP','DonorAge','DonorGender','DonorBloodTypeA','DonorBloodTypeB','DonorBloodTypeO','DonorBloodTypeAB','ABOEqual','ABO_Compatible','ABO_Incompatible','HEP_C_Antigen','DonorDiabetes', 'Distance','DaysInState1','DaysInState1A','DaysInState2','DaysInState1B','BMI','DonorBMI','VADAssist','TotalArtificialHeart','Inotropic','A_Mismatch','B_Mismatch','DR_Mismatch','Height_Difference','Weight_Difference','LVAD_at_listing','LVAD_while_listed','LVAD']\n",
        "        data ['Surv_Name'] = ['Survival_Time', 'Eventful', 'Tranplant_Year']\n",
        "        num_rows = 60400\n",
        "    \n",
        "    else:\n",
        "        data = sio.loadmat('UNOS_Data_Share/Preprocessed_Data/Waitlist_Data')\n",
        "        data['Feature_Name'] = ['Age','Gender', 'Diabetes','PreviousTransplant', 'NumberOfPreviousTransplant', 'VentilatorAssist','ECMOAssist', 'Creatinine', 'BloodTypeA','BloodTypeB','BloodTypeO','BloodTypeAB', 'IABP','DaysInState1', 'DaysInState1A','DaysInState2','DaysInState1B', 'BMI','VADAssist', 'TotalArtificialHeart','Inotropic','LVAD_at_listing','LVAD_while_listed','LVAD']                           \n",
        "        data['Surv_Name'] = ['Survival_Time', 'Eventful', 'Listed_Year']\n",
        "        num_rows = 36329\n",
        "    \n",
        "    #Instead of having the column censored, we will have the column eventful, which will assume value\n",
        "    #1 if the event has been registered and 0 otherwise\n",
        "    for i in range(len(data['Surv'][:, 1])):\n",
        "        if (data['Surv'][i][1] == 0): \n",
        "            data['Surv'][i][1] = 1\n",
        "        else:\n",
        "            data['Surv'][i][1] = 0\n",
        "    \n",
        "    pd_data = pd.DataFrame(data = data['Feature'], index=np.arange(num_rows), columns=data['Feature_Name'])\n",
        "    \n",
        "    surv_data = pd.DataFrame(data = data['Surv'], index=np.arange(num_rows), columns=np.transpose(data['Surv_Name']))\n",
        "    \n",
        "    pd_data = pd.concat([pd_data, surv_data], axis = 1)\n",
        "    \n",
        "    return pd_data\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Input: Dataframe\n",
        "Ouput: np array, standard scaler (the np array is already scaled)\n",
        "\"\"\"\n",
        "def normalize_data(data): \n",
        "    npdata = data.values\n",
        "    npdata = npdata.astype(float)\n",
        "    scaler = preprocessing.StandardScaler().fit(npdata)\n",
        "    npdata = scaler.transform(npdata)\n",
        "    return npdata, scaler\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "def delete_features(npdata, features_to_delete, dataset):\n",
        "    if (dataset != 'nwtco'):\n",
        "        for feature in reversed(features_to_delete):\n",
        "            npdata = np.delete(npdata, [feature], axis = 1)\n",
        "    else:\n",
        "        for feature in (features_to_delete):\n",
        "            npdata = np.delete(npdata, [feature], axis = 1)\n",
        "    return npdata\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Input: string (dataset name)\n",
        "Output: list of two numbers: cens feat and surv feat \n",
        "\"\"\"\n",
        "\n",
        "def get_surv_cens_numbers(dataset): \n",
        "    if (dataset== 'myData'):\n",
        "        surv = 0\n",
        "        cens = 1\n",
        "    if (dataset == 'nwtco'):\n",
        "        surv = 6\n",
        "        cens = 5\n",
        "    elif(dataset == 'aids2'):\n",
        "        surv = 1\n",
        "        cens = 2\n",
        "    elif(dataset == 'flchain' ):\n",
        "        surv = 8\n",
        "        cens = 9\n",
        "    elif(dataset == 'Transplant'):\n",
        "        surv = 50\n",
        "        cens = 51\n",
        "    elif(dataset == 'Waitlist'):\n",
        "        surv = 24\n",
        "        cens = 25\n",
        "    return surv, cens\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Input: string(dataset name)\n",
        "Ouput: two strings, name of the columns containing surv and cens \n",
        "\"\"\"\n",
        "def get_surv_cens_names(dataset):\n",
        "    if (dataset== 'myData'):\n",
        "        surv = 'tte0'\n",
        "        cens = 'event'\n",
        "    if (dataset == 'nwtco'):\n",
        "        surv = 'edrel'\n",
        "        cens = 'rel'\n",
        "    elif(dataset == 'aids2'):\n",
        "        surv = 'death'\n",
        "        cens = 'status'\n",
        "    elif(dataset == 'flchain'):\n",
        "        surv = 'futime'\n",
        "        cens = 'death'\n",
        "    elif(dataset == 'Transplant'):\n",
        "        surv = 'Survival_Time'\n",
        "        cens = 'Eventful'\n",
        "    elif(dataset == 'Waitlist'):\n",
        "        surv = 'Survival_Time'\n",
        "        cens = 'Eventful'\n",
        "    return surv, cens\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Input: array with all the survival times \n",
        "       float with the interval length desired (granularity of the discretization)\n",
        "Ouput: matrix with dimensions: (num_patients, total number of intervals cosidered),\n",
        "       each cell (i, j) in the matrix will have value 1 if the event of death for the \n",
        "       patient i happens in the time interval j\n",
        "\"\"\"\n",
        "def get_event_uncensored_array(survival_times_array, uncensored_array, index_list, intervals_length,min_days,  max_days):\n",
        "\n",
        "    #total_intervals = np.amax(survival_times_array) // intervals_length\n",
        "    total_intervals = (max_days-min_days) // intervals_length\n",
        "\n",
        "    y_event = np.zeros((len(survival_times_array), total_intervals))\n",
        "    \n",
        "    for patient, surv_time in enumerate(survival_times_array):\n",
        "        event_interval = surv_time // intervals_length - (min_days // intervals_length)\n",
        "        for i in range(len(y_event[patient])): \n",
        "            #Since with // we know that the result is cut (e.g., 3.9//2 = 1) we know that the event happens in  event_interval (note that we start to count the intervals from 0)\n",
        "            if (i == event_interval and uncensored_array[index_list[patient]] == 1): \n",
        "                y_event[patient][i] = 1\n",
        "    return y_event\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Input: array with all the survival times \n",
        "       float with the interval length desired (granularity of the discretization)\n",
        "Ouput: matrix with dimensions: (num_patients, total number of intervals cosidered),\n",
        "       each cell (i, j) in the matrix will have value 1 if the \n",
        "       patient i is still alive in the time interval j\n",
        "\"\"\"\n",
        "def get_survivor_array(survival_times_array, intervals_length, min_days, max_days):\n",
        "    \n",
        "    #total_intervals = np.amax(survival_times_array) // intervals_length\n",
        "    total_intervals = (max_days-min_days) // intervals_length\n",
        "    y_survivor = np.zeros((len(survival_times_array), total_intervals))\n",
        "       \n",
        "    for patient, surv_time in enumerate(survival_times_array):\n",
        "        event_interval = surv_time // intervals_length - (min_days // intervals_length)\n",
        "        for i in range(len(y_survivor[patient])): \n",
        "            #Since with // we know that the result is cut (e.g., 3.9//2 = 1) we know that the event happens in  event_interval (note that we start to count the intervals from 0)\n",
        "            if (i < event_interval):\n",
        "                y_survivor[patient][i] = 1\n",
        "    return y_survivor\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Input: array with all the survival times \n",
        "       float with the interval length desired (granularity of the discretization)\n",
        "Ouput: matrix with dimensions: (num_patients, total number of intervals cosidered),\n",
        "       each cell (i, j) in the matrix will have value 1 if the \n",
        "       patient i is  uncensored or if the patient is censored and still alive in the time interval j\n",
        "\"\"\"\n",
        "\n",
        "def get_uncensored_or_survivor_array(survival_times_array, uncensored_array, index_list, intervals_length, min_days,  max_days):\n",
        "    #total_intervals = np.amax(survival_times_array) // intervals_length\n",
        "    total_intervals = (max_days-min_days) // intervals_length\n",
        "    y_uncensored_or_survivor = np.zeros((len(survival_times_array), total_intervals))\n",
        "        \n",
        "    for patient, surv_time in enumerate(survival_times_array):\n",
        "        event_interval = surv_time // intervals_length - (min_days // intervals_length)\n",
        "        for i in range(len(y_uncensored_or_survivor[patient])): \n",
        "            #Since with // we know that the result is cut (e.g., 3.9//2 = 1) we know that the event happens in  event_interval (note that we start to count the intervals from 0)\n",
        "            if (i < event_interval):\n",
        "                y_uncensored_or_survivor[patient][i] = 1\n",
        "           \n",
        "            elif(uncensored_array[index_list[patient]] == 1):\n",
        "                y_uncensored_or_survivor[patient][i] = 1\n",
        "                \n",
        "    return y_uncensored_or_survivor\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Input: array with all the survival times \n",
        "       float with the interval length desired (granularity of the discretization)\n",
        "Ouput: matrix with dimensions: (num_patients, total number of intervals cosidered),\n",
        "       each cell (i, j) in the matrix will have value 1 if the event of death for the \n",
        "       patient i happens in the time interval j\n",
        "\"\"\"\n",
        "def get_event_censored_array(survival_times_array, uncensored_array, index_list, intervals_length, min_days, max_days):\n",
        "\n",
        "    total_intervals = (max_days-min_days) // intervals_length\n",
        "\n",
        "    y_event = np.zeros((len(survival_times_array), total_intervals))\n",
        "    \n",
        "    for patient, surv_time in enumerate(survival_times_array):\n",
        "        event_interval = surv_time // intervals_length -(min_days // intervals_length)\n",
        "        for i in range(len(y_event[patient])): \n",
        "            #Since with // we know that the result is cut (e.g., 3.9//2 = 1) we know that the event happens in  event_interval (note that we start to count the intervals from 0)\n",
        "            if (i == event_interval and uncensored_array[index_list[patient]] == 0): \n",
        "                y_event[patient][i] = 1\n",
        "    return y_event\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Input: pandas dataseframe with data\n",
        "       integer with length of the intervals\n",
        "       integer with the first day considered\n",
        "       integer with the maximum time considered\n",
        "Output: np array transfromed to be inputed in the RNN\n",
        "\"\"\"\n",
        "\n",
        "def create_RNN_input(data, intervals_length, min_days, max_days): \n",
        "    total_intervals = (max_days - min_days) // intervals_length\n",
        "    x_input = []\n",
        "    for i in range(total_intervals):\n",
        "        temp = np.full((len(data), 1), float(i)/total_intervals)\n",
        "        #We now sobstitute the index of the time interval with all the same number \n",
        "        #temp = np.full((len(data), 1), 1.0)\n",
        "        concatenated = np.concatenate((data, temp), axis = 1)\n",
        "        x_input.append(concatenated)  \n",
        "    np_x_input = np.asarray(x_input)\n",
        "    np_x_input = np.transpose(np_x_input, (1,2,0))\n",
        "    return np_x_input\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Input: Dataframe with the data\n",
        "Ouput: Dataframe with preprocessed data\n",
        "\"\"\"\n",
        "\n",
        "def aids2_preprocessing(data):\n",
        "    data = data.replace({'sex': {'M': -1.0, 'F': 1.0}})\n",
        "    data = data.replace({'status':{'D': 1.0, 'A': 0.0}})\n",
        "    data['death'] = data['death'] - data['diag']\n",
        "    del data['diag']\n",
        "    data.rename(columns={'T.categ': 'categ'}, inplace=True)\n",
        "    data=pd.get_dummies(data)\n",
        "    return data\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Input: Dataframe with missing data\n",
        "Output: Dataframe with completed data\n",
        "\"\"\"\n",
        "def flchain_fillMissing(data):\n",
        "    data.rename(columns={'sample.yr': 'sample_year'}, inplace=True)\n",
        "    data.rename(columns={'flc.grp': 'flc_grp'}, inplace=True)\n",
        "    data['age'].fillna(data.mean()['age'], inplace=True)\n",
        "    data['sample_year'].fillna(data.mean()['sample_year'] , inplace=True)\n",
        "    data['kappa'].fillna(data.mean()['kappa'], inplace=True)\n",
        "    data['lambda'].fillna(data.mean()['lambda'], inplace=True)\n",
        "    data['flc_grp'].fillna(data.mean()['flc_grp'], inplace=True)\n",
        "    data['creatinine'].fillna(data.mean()['creatinine'], inplace=True) \n",
        "    data['futime'].fillna(data.mean()['futime'], inplace=True)\n",
        "    \n",
        "    data['sex'].fillna(data['sex'].value_counts().index[0], inplace=True)\n",
        "    data['death'].fillna(data['death'].value_counts().index[0], inplace=True)\n",
        "    data['chapter'].fillna(data['chapter'].value_counts().index[0], inplace=True)\n",
        "    data['mgus'].fillna(data['mgus'].value_counts().index[0], inplace=True)\n",
        "    return data\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Input: Dataframe\n",
        "Output preprocessed Dataframe\n",
        "\"\"\"\n",
        "def flchain_preprocessing(data): \n",
        "    data = data.replace({'sex': {'M': -1.0, 'F': 1.0}})\n",
        "    data = pd.get_dummies(data)\n",
        "    \n",
        "    return data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_shuffled_data(dataset):\n",
        "    if (dataset == 'Transplant' or dataset == 'Waitlist'):\n",
        "        data = import_UNOS_dataset(dataset)\n",
        "    else:\n",
        "        data = import_small_dataset(dataset)\n",
        "           \n",
        "    data = data.sample(frac=1).reset_index(drop=True)\n",
        "    data = shuffle(data)\n",
        "    \n",
        "    return data\n",
        "    \n",
        "\n",
        "\"\"\"\n",
        "Input: int (intervals length)\n",
        "       int (first days considered)\n",
        "       int (max days considered)\n",
        "       string (dataset name)\n",
        "Output: Dictionary with all the data ready to be used\n",
        "\"\"\"\n",
        "\n",
        "def elaborate_data(data, intervals_length, min_days, max_days_considered, dataset, cross_validation_number): \n",
        "    \n",
        "    if (dataset == 'aids2'):\n",
        "        data = aids2_preprocessing(data)\n",
        "    elif(dataset == 'flchain'):\n",
        "        data = flchain_fillMissing(data)\n",
        "        data = flchain_preprocessing(data)   \n",
        "        \n",
        "    for k in range(cross_validation_number):\n",
        "        temp1, temp2 = np.split(data, [int(0.80*len(data))])\n",
        "        frames = [temp2, temp1]\n",
        "        data = pd.concat(frames)\n",
        "          \n",
        "    data, Val, Test = np.split(data, [int(0.60*len(data)), int(0.80*len(data))])\n",
        "    \n",
        "    surv_feature, cens_feature = get_surv_cens_names(dataset)\n",
        "    idx_features_to_delete = get_surv_cens_numbers(dataset)\n",
        "    \n",
        "    \n",
        "    y_survivor = get_survivor_array(survival_times_array=data[surv_feature], intervals_length=intervals_length, min_days = min_days, max_days = max_days_considered)\n",
        "    y_Val_survivor = get_survivor_array(survival_times_array= Val[surv_feature], intervals_length=intervals_length, min_days = min_days, max_days = max_days_considered)\n",
        "    y_Test_survivor = get_survivor_array(survival_times_array=Test[surv_feature], intervals_length=intervals_length, min_days = min_days, max_days = max_days_considered)\n",
        "\n",
        "    #We have to pass also the index list because when we drop the rows we drop also the respective indexes\n",
        "    y_event = get_event_uncensored_array(survival_times_array=data[surv_feature], uncensored_array=data[cens_feature], index_list = data.index.values, intervals_length=intervals_length, min_days = min_days, max_days = max_days_considered)\n",
        "    y_event_censoring = get_event_uncensored_array(survival_times_array=data[surv_feature], uncensored_array=data[cens_feature], index_list = data.index.values, intervals_length=intervals_length, min_days = min_days, max_days = max_days_considered)\n",
        "\n",
        "    y_uncensored_or_survivor = get_uncensored_or_survivor_array(survival_times_array=data[surv_feature],uncensored_array=data[cens_feature], index_list = data.index.values, intervals_length=intervals_length, min_days = min_days, max_days = max_days_considered)\n",
        "    y_uncensored_or_survivor_val = get_uncensored_or_survivor_array(survival_times_array=Val[surv_feature],uncensored_array=Val[cens_feature], index_list = Val.index.values, intervals_length=intervals_length, min_days = min_days, max_days = max_days_considered)\n",
        "    y_uncensored_or_survivor_test = get_uncensored_or_survivor_array(survival_times_array=Test[surv_feature],uncensored_array=Test[cens_feature], index_list = Test.index.values, intervals_length=intervals_length, min_days = min_days, max_days = max_days_considered)\n",
        "    \n",
        "    npdata, scaler = normalize_data(data)\n",
        "    npdata = delete_features(npdata, idx_features_to_delete, dataset)\n",
        "\n",
        "    np_x_input = create_RNN_input(data=npdata, intervals_length=intervals_length,min_days = min_days,  max_days = max_days_considered)\n",
        "   \n",
        "    data_test = Test\n",
        "    data_val = Val\n",
        "    \n",
        "    Val = Val.values\n",
        "    Val = Val.astype(float) \n",
        "    \n",
        "    Test = Test.values\n",
        "    Test = Test.astype(float) \n",
        "    \n",
        "    Val = scaler.transform(Val)\n",
        "    Test = scaler.transform(Test)\n",
        "    \n",
        "    Val = delete_features(Val, idx_features_to_delete, dataset)\n",
        "    Test = delete_features(Test, idx_features_to_delete, dataset)\n",
        "\n",
        "    np_val_input = create_RNN_input(data = Val, intervals_length=intervals_length,min_days = min_days, max_days = max_days_considered)\n",
        "    np_test_input = create_RNN_input(data = Test, intervals_length=intervals_length, min_days = min_days, max_days = max_days_considered)\n",
        "\n",
        "    data = np.asarray(data)\n",
        "    data_test = np.asarray(data_test)\n",
        "    data_val = np.asarray(data_val)\n",
        "    \n",
        "    return dict(\n",
        "            X_data = data,\n",
        "            X_data_test = data_test,\n",
        "            X_data_val = data_val, \n",
        "            y_survivor = y_survivor,\n",
        "            y_Val_survivor = y_Val_survivor, \n",
        "            y_Test_survivor = y_Test_survivor,\n",
        "            y_event = y_event,\n",
        "            y_event_censoring = y_event_censoring,\n",
        "            y_uncensored_or_survivor = y_uncensored_or_survivor, \n",
        "            y_uncensored_or_survivor_val = y_uncensored_or_survivor_val, \n",
        "            y_uncensored_or_survivor_test = y_uncensored_or_survivor_test, \n",
        "            rnn_input = np_x_input,\n",
        "            rnn_val = np_val_input,\n",
        "            rnn_test = np_test_input\n",
        "            )\n",
        " \n",
        "    \n",
        "\"\"\"\n",
        "Input: numpy matrix\n",
        "       string (dataset name)\n",
        "Output: mask of the acceptable pairs\n",
        "\"\"\"\n",
        "def get_accettablePairsMsk(data, dataset): \n",
        "    acc_pairs_matrix = []\n",
        "    surv_feat, cens_feat = get_surv_cens_numbers(dataset)\n",
        "    \n",
        "    for surv_i, compl_i in zip(data[:, surv_feat], data[:, cens_feat]): \n",
        "        acc_pairs_row = np.zeros(len(data[:, surv_feat]))\n",
        "        if (compl_i == 1):\n",
        "            j = 0\n",
        "            for surv_j, compl_j in zip(data[:, surv_feat], data[:, cens_feat]):\n",
        "                if (compl_j == 1 or surv_j >= surv_i):\n",
        "                    acc_pairs_row[j] = 1\n",
        "                j += 1\n",
        "        acc_pairs_matrix.append(acc_pairs_row)\n",
        "    \n",
        "    acc_pairs_matrix = np.asarray(acc_pairs_matrix)\n",
        "    return acc_pairs_matrix\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install lifelines"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0HkWKeFo1J0",
        "outputId": "94a9f28d-2092-45c1-fef5-1ebd47d5cd95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting lifelines\n",
            "  Downloading lifelines-0.27.4-py3-none-any.whl (349 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m349.7/349.7 KB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting formulaic>=0.2.2\n",
            "  Downloading formulaic-0.5.2-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.1/77.1 KB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=3.0 in /usr/local/lib/python3.9/dist-packages (from lifelines) (3.5.3)\n",
            "Requirement already satisfied: autograd>=1.5 in /usr/local/lib/python3.9/dist-packages (from lifelines) (1.5)\n",
            "Collecting autograd-gamma>=0.3\n",
            "  Downloading autograd-gamma-0.5.0.tar.gz (4.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.9/dist-packages (from lifelines) (1.22.4)\n",
            "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from lifelines) (1.3.5)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from lifelines) (1.10.1)\n",
            "Requirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.9/dist-packages (from autograd>=1.5->lifelines) (0.16.0)\n",
            "Collecting astor>=0.8\n",
            "  Downloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: wrapt>=1.0 in /usr/local/lib/python3.9/dist-packages (from formulaic>=0.2.2->lifelines) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.9/dist-packages (from formulaic>=0.2.2->lifelines) (4.5.0)\n",
            "Collecting interface-meta>=1.2.0\n",
            "  Downloading interface_meta-1.3.0-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.0->lifelines) (4.39.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.0->lifelines) (8.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.0->lifelines) (2.8.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.0->lifelines) (23.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.0->lifelines) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.0->lifelines) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.0->lifelines) (3.0.9)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.0.0->lifelines) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0->lifelines) (1.15.0)\n",
            "Building wheels for collected packages: autograd-gamma\n",
            "  Building wheel for autograd-gamma (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for autograd-gamma: filename=autograd_gamma-0.5.0-py3-none-any.whl size=4048 sha256=edae9852b0523b7dc5f9c81774391869cd479eb6ada79d623d9c3cfe6c31d5ae\n",
            "  Stored in directory: /root/.cache/pip/wheels/a8/03/64/8557323821d25118c3a2dc1646996f7a962a8970d4b7d22473\n",
            "Successfully built autograd-gamma\n",
            "Installing collected packages: interface-meta, astor, autograd-gamma, formulaic, lifelines\n",
            "Successfully installed astor-0.8.1 autograd-gamma-0.5.0 formulaic-0.5.2 interface-meta-1.3.0 lifelines-0.27.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#%% %pip install 'lifelines'\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "import tensorflow as tf2\n",
        "import numpy as np\n",
        "#import data_handler\n",
        "from lifelines.utils import concordance_index\n",
        "\n",
        "#%%CLASS RNN\n",
        "\n",
        "\n",
        "class RNN_SURV():\n",
        "    \n",
        "    \n",
        "    def __init__(self,\n",
        "                 bidirectional,\n",
        "                 cell_type,\n",
        "                 constant_loss,\n",
        "                 constant_raykar,\n",
        "                 learning_rate,\n",
        "                 num_features, \n",
        "                 num_rnn_layers,\n",
        "                 num_nodes_h1, \n",
        "                 num_nodes_h2,\n",
        "                 num_nodes_input_rnn_layer,\n",
        "                 num_steps,\n",
        "                 state_size\n",
        "                 ): \n",
        "        self.bidirectional = bidirectional\n",
        "        self.cell_type = cell_type\n",
        "        \n",
        "        self.constant_loss = constant_loss\n",
        "        self.constant_raykar = constant_raykar\n",
        "        \n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_rnn_layers = num_rnn_layers\n",
        "        self.num_nodes_input = num_features\n",
        "        self.num_nodes_h1 = num_nodes_h1\n",
        "        self.num_nodes_h2 = num_nodes_h2\n",
        "        self.num_nodes_input_rnn_layer = num_nodes_input_rnn_layer\n",
        "        self.num_nodes_output = num_steps\n",
        "        self.state_size = state_size\n",
        "        \n",
        "        self.X = 0\n",
        "        self.Y_true = 0\n",
        "        self.dropout_hid_state = 1\n",
        "        self.dropout_keep_prob_output = 1\n",
        "        self.dropout_keep_prob_input = 1\n",
        "        self.dropout_keep_prob_state = 1\n",
        "        \n",
        "        return\n",
        "    \n",
        "    \n",
        "    \n",
        "    \"\"\"\n",
        "    Input: - y_pred: survival predictions made by the neural network\n",
        "           - survival_vector: true labels (each element of the vector is equal to 1 if the patient is still alive, and 0 if the patient is dead/lost)\n",
        "           - uncensored_or_survivor: censored labels (each element of the vector is equal to 1 for al those intervals of which we know the real outcome, and 0 for those intervals in which we lost track of the patient)\n",
        "           - btach_size: size of the input batch\n",
        "    Output: sum over the values of the loss functions in the different time intervals\n",
        "    \"\"\"\n",
        "    def PartialLikelihood(self, y_pred, survival_vector, uncensored_or_survivor, batch_size): \n",
        "        \n",
        "        y_pred = tf.unstack(y_pred, axis = 1)[0]\n",
        "        losses =  tf.multiply(uncensored_or_survivor, tf.nn.sigmoid_cross_entropy_with_logits(labels=survival_vector, logits = y_pred))\n",
        "        interval_loss = tf.reduce_sum(losses) \n",
        "        \n",
        "        return interval_loss\n",
        "    \n",
        "    \n",
        "    \n",
        "    \"\"\"\n",
        "    Input: - mask_acc_pairs: mask to take into consideration only acceptable pairs\n",
        "           - output: risk score (final neural network output)\n",
        "    Output: value of the raykar loss function (defined in equation (6) in the paper)\n",
        "    \"\"\" \n",
        "    def RaykarLikelihood(self, msk_acc_pairs, output): \n",
        "        output_col_concat = tf.concat([output, output], axis = 1)\n",
        "        for i in range(self.input_length-2): \n",
        "            output_col_concat = tf.concat([output_col_concat, output], axis = 1 )\n",
        "        output_j = output_col_concat\n",
        "        output_i = tf.transpose(output_col_concat)\n",
        "        loss = tf.reduce_sum(tf.multiply(msk_acc_pairs, (1+ tf.log(tf.nn.sigmoid(tf.subtract(output_j, output_i))+ 1e-30)/tf.log(2.0))))\n",
        "        loss = loss / tf.reduce_sum(msk_acc_pairs)\n",
        "        return -loss\n",
        "    \n",
        "    \n",
        "    \n",
        "    \"\"\"\n",
        "    Input: - batch_size\n",
        "    Output: the object itself\n",
        "    \n",
        "    This function defines the graph of RNN_SURV \n",
        "    \"\"\"\n",
        "    def build_model(self, batch_size): \n",
        "        tf.reset_default_graph()\n",
        "        tf.set_random_seed(1)\n",
        "        \n",
        "        #Placeholders########################\n",
        "        self.raw_X = tf.placeholder(tf.float32, [None, None, self.num_nodes_input], name='input_placeholder')\n",
        "        self.survivor = tf.placeholder(tf.float32, [None, None], name='survivor_placeholder')\n",
        "        self.event = tf.placeholder(tf.float32, [None, None], name='event_placeholder')\n",
        "        self.event_censoring = tf.placeholder(tf.float32, [None, None], name='event_placeholder_censoring')\n",
        "        self.uncensored_or_survivor=tf.placeholder(tf.float32, [None, None], name='uncensored_or_survivor_placeholder')\n",
        "        self.dropout_hid_state = tf.placeholder(tf.float32, name = 'dropout_hidden_layer')\n",
        "        self.dropout_keep_prob_output = tf.placeholder(tf.float32, name = 'dropout_keep_prob_output')\n",
        "        self.dropout_keep_prob_input = tf.placeholder(tf.float32, name = 'dropout_keep_prob_input')\n",
        "        self.dropout_keep_prob_state = tf.placeholder(tf.float32, name='dropout_keep_prob_state')\n",
        "        \n",
        "        self.msk_acc_pairs = tf.placeholder(tf.float32, [None, None], name = 'msk_acc_pairs_placeholder')\n",
        "        self.input_length = batch_size\n",
        "                \n",
        "        # Define weights\n",
        "        if (self.bidirectional == False):\n",
        "            self.weights = {\n",
        "                'embed_1': tf.Variable(tf.truncated_normal([self.num_nodes_input, self.num_nodes_h1]), name='W_embed_1'), \n",
        "                'embed_2': tf.Variable(tf.truncated_normal([self.num_nodes_h1, self.num_nodes_h2]), name='W_embed_2'), \n",
        "                'input_rnn' : tf.Variable(tf.truncated_normal([self.num_nodes_h2, self.num_nodes_input_rnn_layer]), name='W_input_rnn'), \n",
        "                'out': tf.Variable(tf.truncated_normal([self.state_size, 1]), name=\"W_ois_trainingut\"),\n",
        "                'cox_out': tf.Variable(tf.truncated_normal([self.num_nodes_output, 1]), name=\"W_cox_output\")\n",
        "            }\n",
        "        else:\n",
        "            self.weights = {\n",
        "                'embed_1': tf.Variable(tf.truncated_normal([self.num_nodes_input, self.num_nodes_h1]), name='W_bid_embed_1'), \n",
        "                'embed_2': tf.Variable(tf.truncated_normal([self.num_nodes_h1, self.num_nodes_h2]), name='W_bid_embed_2'), \n",
        "                'input_rnn': tf.Variable(tf.truncated_normal([self.num_nodes_h2, self.num_nodes_input_rnn_layer]), name='W_bid_input_rnn'), \n",
        "                'out': tf.Variable(tf.truncated_normal([self.state_size*2, 1]), name=\"W_out\"),\n",
        "                'cox_out': tf.Variable(tf.truncated_normal([self.num_nodes_output, 1]), name=\"W_cox_output\")\n",
        "\n",
        "            }\n",
        "        biases = {\n",
        "    \n",
        "            'embed_1' : tf.Variable(tf.zeros([self.num_nodes_h1]), name=\"b_embed_1\"),\n",
        "            'embed_2' : tf.Variable(tf.zeros([self.num_nodes_h2]), name=\"b_embed_2\"),\n",
        "            'input_rnn' : tf.Variable(tf.zeros([self.num_nodes_input_rnn_layer]), name=\"b_input_rnn\"),\n",
        "            'out': tf.Variable(tf.zeros([1]), name=\"b_out\"),\n",
        "            'cox_out': tf.Variable(tf.zeros([1]), name=\"b_cox_out\")\n",
        "        }\n",
        "        ##################################\n",
        "        #tf.compat.v1.nn.rnn_cell.MultiRNNCell\n",
        "        #tf.keras.layers.StackedRNNCells\n",
        "        #tf.compat.v1.nn.rnn_cell.LSTMCell\n",
        "        def get_RNN_cell():\n",
        "            cell = tf.nn.rnn_cell.BasicRNNCell(num_units = self.state_size)\n",
        "            cell = tf.nn.rnn_cell.DropoutWrapper(cell, input_keep_prob = self.dropout_keep_prob_input, output_keep_prob=self.dropout_keep_prob_output, state_keep_prob = self.dropout_keep_prob_state)\n",
        "            return cell\n",
        "        \n",
        "        def get_LSTM_cell(state_size): \n",
        "            cell = tf.nn.rnn_cell.LSTMCell(state_size, state_is_tuple=True)\n",
        "            cell = tf.nn.rnn_cell.DropoutWrapper(cell, input_keep_prob = self.dropout_keep_prob_input, output_keep_prob=self.dropout_keep_prob_output, state_keep_prob = self.dropout_keep_prob_state)\n",
        "            print(\"LSTM cell\")\n",
        "            return cell\n",
        "        \n",
        "        def get_GRU_cell(state_size): \n",
        "            cell =  tf.nn.rnn_cell.GRUCell(state_size)\n",
        "            cell = tf.nn.rnn_cell.DropoutWrapper(cell, input_keep_prob = self.dropout_keep_prob_input, output_keep_prob=self.dropout_keep_prob_output, state_keep_prob = self.dropout_keep_prob_state)\n",
        "            return cell\n",
        "        \n",
        "        def get_RNN_layers(X, weights, biases, cell_type=\"LSTM\"):\n",
        "            #rnn_inputs is a list of num_steps tensors with shape [batch_size, num_features]\n",
        "            if (self.bidirectional == False):\n",
        "                if (cell_type==\"LSTM\"):\n",
        "                    \n",
        "                    cell = tf.nn.rnn_cell.MultiRNNCell([get_LSTM_cell(self.state_size) for size in range(self.num_rnn_layers)], state_is_tuple=True)\n",
        "                    \n",
        "                elif (cell_type==\"GRU\"):\n",
        "                    cell = tf.nn.rnn.MultiRNNCell([get_GRU_cell(self.state_size) for _ in range(self.num_rnn_layers)], state_is_tuple=True)\n",
        "                elif(cell_type == \"RNN\"):\n",
        "                    cell = tf.nn.rnn.MultiRNNCell([get_RNN_cell(self.state_size) for _ in range(self.num_rnn_layers)], state_is_tuple=True)\n",
        "                else: \n",
        "                    print(\"Invalid argument\")\n",
        "                return cell\n",
        "            else:\n",
        "                if (cell_type == \"LSTM\"):\n",
        "                    fwd_cell = get_LSTM_cell(self.state_size)\n",
        "                    bwd_cell = get_LSTM_cell(self.state_size)\n",
        "                elif (cell_type == \"GRU\"):\n",
        "                    fwd_cell = tf.nn.rnn.MultiRNNCell([get_GRU_cell(self.state_size) for _ in range(self.num_rnn_layers)], state_is_tuple=True)\n",
        "                    bwd_cell = tf.nn.rnn.MultiRNNCell([get_GRU_cell(self.state_size) for _ in range(self.num_rnn_layers)], state_is_tuple=True)\n",
        "                else: \n",
        "                    print(\"Invalid argument\")\n",
        "                return fwd_cell, bwd_cell\n",
        "########################\n",
        "        unstacked_raw_X = tf.unstack(self.raw_X, self.num_nodes_output, axis =1)\n",
        "    \n",
        "        X = []\n",
        "        \n",
        "        for step_X in unstacked_raw_X: \n",
        "            linear_h_1 = tf.matmul(step_X, self.weights['embed_1']) + biases['embed_1']\n",
        "            h_1 = tf.nn.relu(linear_h_1)\n",
        "            d_1 = tf.nn.dropout(h_1, keep_prob = self.dropout_hid_state)\n",
        "            d_1 =  d_1 / self.dropout_hid_state \n",
        "            linear_h_2 = tf.matmul(d_1, self.weights['embed_2']) + biases['embed_2']\n",
        "            h_2 = tf.nn.relu(linear_h_2)\n",
        "            d_2 = tf.nn.dropout(h_2, keep_prob = self.dropout_hid_state)\n",
        "            d_2 =  d_2 / self.dropout_hid_state \n",
        "\n",
        "            linear_h_3 = tf.nn.relu(tf.matmul(d_2, self.weights['input_rnn']) + biases['input_rnn'])\n",
        "            h_3 = tf.nn.relu(linear_h_3)\n",
        "            d_3 =  tf.nn.dropout(h_3, keep_prob = self.dropout_hid_state)\n",
        "            d_3 =  d_3 / self.dropout_hid_state \n",
        "\n",
        "            X.append(d_3)\n",
        "    \n",
        "        X = tf.stack(X, axis = 0)\n",
        "        \n",
        "        X.set_shape([None, None, self.num_nodes_input_rnn_layer])\n",
        "        X = tf.transpose(X, [1, 0, 2])\n",
        "        \n",
        "        cell = get_RNN_layers(X, self.weights, biases, cell_type=self.cell_type)\n",
        "    \n",
        "        def execute_rnn(cell, X_input, init_state_fw, init_state_bw=None):\n",
        "            if (self.bidirectional == False):\n",
        "                rnn_outputs, final_state =  tf.nn.dynamic_rnn(cell, X_input, initial_state = init_state_fw, dtype=tf.float32)\n",
        "            else:\n",
        "                rnn_outputs, final_state = tf.nn.bidirectional_dynamic_rnn(cell_fw = cell[0], cell_bw = cell[1], inputs = X_input, initial_state_fw = init_state_fw, initial_state_bw=init_state_bw, dtype=tf.float32)\n",
        "            return rnn_outputs, final_state\n",
        "        \n",
        "            \n",
        "        if(self.bidirectional == False):\n",
        "            init_state_fw = cell.zero_state(batch_size, tf.float32)\n",
        "            init_state_bw = None\n",
        "            rnn_outputs, final_state = execute_rnn(cell, X, init_state_fw)\n",
        "        else:\n",
        "            init_state_fw = cell[0].zero_state(batch_size, tf.float32)\n",
        "            init_state_bw = cell[1].zero_state(batch_size, tf.float32)\n",
        "            rnn_outputs, final_state = execute_rnn(cell, X, init_state_fw, init_state_bw)\n",
        "        \n",
        "        \n",
        "        if (self.bidirectional == True):\n",
        "            rnn_outputs = tf.concat([rnn_outputs[0], rnn_outputs[1]], axis = 2)\n",
        "            rnn_outputs = tf.reshape(rnn_outputs, [-1, self.state_size*2])\n",
        "        else:\n",
        "            rnn_outputs = tf.reshape(rnn_outputs, [-1, self.state_size])\n",
        "            \n",
        "        logits = tf.matmul(rnn_outputs, self.weights['out']) + biases['out']\n",
        "        self.line_predictions = tf.nn.sigmoid(logits)\n",
        "        \n",
        "        matrix_logits = tf.reshape(logits, [-1, self.num_nodes_output])\n",
        "        self.cox_output = (tf.matmul(matrix_logits, self.weights['cox_out']) + biases['cox_out'])\n",
        "        \n",
        "        line_survivor = tf.reshape(self.survivor, [-1])\n",
        "        line_uncensored_or_survivor = tf.reshape(self.uncensored_or_survivor, [-1])\n",
        "        \n",
        "        self.loss = RNN_SURV.PartialLikelihood(self, y_pred = logits, survival_vector = line_survivor, uncensored_or_survivor = line_uncensored_or_survivor, batch_size = batch_size)\n",
        "        self.raykar_loss = RNN_SURV.RaykarLikelihood(self, msk_acc_pairs = self.msk_acc_pairs, output = self.cox_output)\n",
        "        \n",
        "        L2_regularizer_weights = tf.nn.l2_loss(self.weights['embed_1']) + tf.nn.l2_loss(self.weights['embed_2'])+ tf.nn.l2_loss(self.weights['input_rnn'])+ tf.nn.l2_loss(self.weights['out'])\n",
        "        L2_regularizer_biases = tf.nn.l2_loss(biases['embed_1']) + tf.nn.l2_loss(biases['embed_2'])+ tf.nn.l2_loss(biases['input_rnn']) + tf.nn.l2_loss(biases['out'])\n",
        "    \n",
        "        self.total_loss = tf.reduce_sum(self.constant_raykar*self.raykar_loss + self.constant_loss*self.loss + 0.1* L2_regularizer_weights + 0.1 * L2_regularizer_biases) #+ 1 * tf.nn.l2_loss(biases['cox_out']) +1 *  tf.nn.l2_loss(weights['cox_out']))\n",
        "        #print(self.total_loss.shape)\n",
        "        self.train_step = tf.train.AdamOptimizer(self.learning_rate).minimize(self.total_loss)\n",
        "        \n",
        "        self.predictions = tf.nn.sigmoid(matrix_logits)\n",
        "        self.true_labels = tf.cast(self.survivor, tf.int8)\n",
        "        \n",
        "        return self\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    def train_RNN( \n",
        "              self,\n",
        "              batch_size,\n",
        "              data,\n",
        "              dataset,\n",
        "              dropout_hid_state,\n",
        "              dropout_keep_prob_input,\n",
        "              dropout_keep_prob_output,\n",
        "              dropout_keep_prob_state,\n",
        "              num_epochs_per_time,\n",
        "              patience_max, \n",
        "              saved_model = False, \n",
        "              verbose=True):\n",
        "        \n",
        "        path = \"./checkpoints/\"\n",
        "        path += str(dataset)\n",
        "        path += \"/state_size\"\n",
        "        path += str(self.state_size)\n",
        "        if (self.bidirectional == True):\n",
        "            path += \"Bid\"\n",
        "        path += \".ckpt\"\n",
        "        \n",
        "        print(\"path: \", path)\n",
        "       \n",
        "        iteration = 0\n",
        "        patience = patience_max\n",
        "        max_c_index = 0\n",
        "        \n",
        "        \n",
        "        with tf.Session() as sess: \n",
        "                \n",
        "            if (saved_model == False): \n",
        "                print(\"NO SAVED MODEL\")\n",
        "                sess.run(tf.global_variables_initializer())\n",
        "            else:  \n",
        "                restorer = tf.train.Saver()\n",
        "                restorer.restore(sess, path)\n",
        "       \n",
        "            while (patience > 0):\n",
        "                \n",
        "                iteration = iteration + num_epochs_per_time\n",
        "            \n",
        "                for epoch in range(num_epochs_per_time):\n",
        "        \n",
        "                    num_batches = len(data['rnn_input']) // batch_size\n",
        "        \n",
        "                    shuffler = np.random.permutation(len(data['rnn_input']))\n",
        "        \n",
        "                    for num_batch in range(num_batches):\n",
        "                        \n",
        "                        batch_X = data['rnn_input'][shuffler[num_batch*batch_size: num_batch*batch_size+batch_size]]\n",
        "                        batch_X = np.transpose(batch_X, [0,2,1])\n",
        "                        batch_survivor = data['y_survivor'][shuffler[num_batch*batch_size: num_batch*batch_size+batch_size]]\n",
        "                        batch_event = data['y_event'][shuffler[num_batch*batch_size: num_batch*batch_size+batch_size]]\n",
        "                        batch_event_censoring = data['y_event_censoring'][shuffler[num_batch*batch_size: num_batch*batch_size+batch_size]]\n",
        "                        batch_uncensored_or_survivor = data['y_uncensored_or_survivor'][shuffler[num_batch*batch_size: num_batch*batch_size+batch_size]]\n",
        "        \n",
        "                        batch_data = data['X_data'][shuffler[num_batch*batch_size: num_batch*batch_size+batch_size]]\n",
        "    \n",
        "                        batch_msk_acc_pairs = get_accettablePairsMsk(batch_data, dataset = dataset)\n",
        "                                                \n",
        "                        feed_dict = {self.raw_X: batch_X, self.survivor: batch_survivor, self.event:batch_event, self.event_censoring : batch_event_censoring, self.uncensored_or_survivor: batch_uncensored_or_survivor,  self.dropout_hid_state : dropout_hid_state, self.dropout_keep_prob_output : dropout_keep_prob_output, self.dropout_keep_prob_input: dropout_keep_prob_input, self.dropout_keep_prob_state: dropout_keep_prob_state,  self.msk_acc_pairs: batch_msk_acc_pairs}\n",
        "                        \n",
        "                        _, r, l, training_loss_, predictions_ , true_labels_ = sess.run([ self.train_step, self.raykar_loss, self.loss, self.total_loss, self.predictions, self.true_labels], feed_dict)\n",
        "                        saved_model = True\n",
        "                        \n",
        "                \n",
        "                c_index = RNN_SURV.validate(self, batch_size, data, sess, dataset)\n",
        "\n",
        "                if(max_c_index >= c_index):\n",
        "                    patience = patience - 1\n",
        "                else:\n",
        "                    patience = patience_max\n",
        "                    saver = tf.train.Saver()\n",
        "                    saver.save(sess, path)\n",
        "                    print(\"UPDATED\")\n",
        "                    max_c_index = c_index\n",
        "            \n",
        "\n",
        "        return max_c_index\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \"\"\"\n",
        "    Input: - data: actual data\n",
        "           - dataset: string containing the name of the dataset\n",
        "           - phase: it can either be 'validation' or 'test'\n",
        "           \n",
        "    Output: the two columns containing the true values for the time-to-event and the censoring indicator respectively.\n",
        "    To be noted: for every i cens[i] will be equal to 0 if censored and equal to 1 if eventful\n",
        "    \"\"\"\n",
        "    def get_truths_cens(self, data, dataset, phase):\n",
        "        if (phase == 'validation'): \n",
        "            label = 'X_data_val'\n",
        "        elif(phase == 'test'):\n",
        "            label = 'X_data_test'\n",
        "        \n",
        "        if (dataset == 'nwtco'):\n",
        "            truths = data[label][:, 6]\n",
        "            cens = data[label][:, 5]\n",
        "        elif (dataset == 'myData'):\n",
        "            truths = data[label][:, 0]\n",
        "            cens = data[label][:, 1]\n",
        "        elif(dataset == 'aids2'):\n",
        "             truths = data[label][:, 1]\n",
        "             cens = data[label][:, 2]\n",
        "        elif(dataset == 'flchain'):\n",
        "            truths = data[label][:, 8]\n",
        "            cens = data [label][:, 9]\n",
        "        elif(dataset == 'Transplant'):\n",
        "            truths = data[label][:, 50]\n",
        "            cens = data [label][:, 51]\n",
        "        elif(dataset == 'Waitlist'):\n",
        "            truths = data[label][:, 24]\n",
        "            cens = data [label][:, 25]\n",
        "        \n",
        "        return truths, cens\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "    def validate(self, \n",
        "                      batch_size, \n",
        "                      data, val_sess, \n",
        "                      dataset\n",
        "                      ):\n",
        "    \n",
        "        validation_predictions = np.zeros(0)\n",
        "        \n",
        "        num_loops = len(data['y_Val_survivor']) // batch_size\n",
        "        for i in range(num_loops):\n",
        "            Y_val = data['y_Val_survivor'][batch_size*i:batch_size*i+batch_size]\n",
        "            X_val = data['rnn_val'][batch_size*i:batch_size*i+batch_size]\n",
        "            X_val = np.transpose(X_val, [0,2,1])\n",
        "             \n",
        "            feed_dict = {self.raw_X: X_val, self.survivor: Y_val, self.dropout_hid_state:1, self.dropout_keep_prob_output: 1, self.dropout_keep_prob_input: 1, self.dropout_keep_prob_state: 1}\n",
        "            s1 = val_sess.run(self.cox_output, feed_dict)\n",
        "            s1 = s1[:, 0]\n",
        "            s1 = np.asarray(s1)\n",
        "            validation_predictions = np.concatenate([validation_predictions, s1])\n",
        "        \n",
        "        \n",
        "        truths, cens = RNN_SURV.get_truths_cens(self, data, dataset, phase = 'validation')\n",
        "        \n",
        "        C_index_val = concordance_index(truths[:len(validation_predictions)], -validation_predictions, cens[:len(validation_predictions)])\n",
        "        \n",
        "        print(\"c-index_validation: \", C_index_val)\n",
        "\n",
        "        return C_index_val\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    def test(\n",
        "         self,\n",
        "         data,\n",
        "         dataset, \n",
        "         input_size,\n",
        "         is_training\n",
        "         ):\n",
        "    \n",
        "        RNN_SURV.build_model(self, batch_size = input_size)\n",
        "            \n",
        "        with tf.Session() as test_sess: \n",
        "            saver = tf.train.Saver()\n",
        "            path = \"./checkpoints/\"\n",
        "            path += str(dataset)\n",
        "            path += \"/state_size\"\n",
        "            path += str(self.state_size)\n",
        "            if (self.bidirectional == True):\n",
        "                path += \"Bid\"\n",
        "            path += \".ckpt\"\n",
        "            saver.restore(test_sess, path)\n",
        "            \n",
        "            X_test = data['rnn_test']\n",
        "            X_test = np.transpose(X_test, [0, 2, 1])\n",
        "            y_test = data['y_Test_survivor']\n",
        "            \n",
        "            truths, cens = RNN_SURV.get_truths_cens(self, data, dataset, phase = 'test')\n",
        "            \n",
        "            feed_dict = {self.raw_X: X_test, self.survivor: y_test,self.dropout_hid_state: 1, self.dropout_keep_prob_output: 1, self.dropout_keep_prob_input : 1, self.dropout_keep_prob_state : 1}\n",
        "            true_labels_, predictions_, cox_output_ = test_sess.run([self.survivor, self.predictions, self.cox_output], feed_dict)\n",
        "            \n",
        "            C_index_test = concordance_index(truths, -cox_output_, cens)\n",
        "\n",
        "                \t\n",
        "        return C_index_test\n"
      ],
      "metadata": {
        "id": "3UHDCL70FYh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dat=get_shuffled_data('myData')\n",
        "elData=elaborate_data(data= dat, intervals_length=100, min_days=0, max_days_considered=1100, dataset='myData', cross_validation_number=5)"
      ],
      "metadata": {
        "id": "Moq7abHBeODO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rnnNet=RNN_SURV(bidirectional=False,\n",
        "          cell_type=\"LSTM\",\n",
        "          constant_loss=0.5,\n",
        "          constant_raykar=0.5,\n",
        "          learning_rate=0.01,\n",
        "          num_features=12, \n",
        "          num_rnn_layers=2,\n",
        "          num_nodes_h1=15, \n",
        "          num_nodes_h2=15,\n",
        "          num_nodes_input_rnn_layer=18,\n",
        "          num_steps=11 ,\n",
        "          state_size=3\n",
        "                 )"
      ],
      "metadata": {
        "id": "oUErxznoV0tO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rnnNet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wf4X6qbE8XxI",
        "outputId": "ec306908-f37d-41f6-cda0-0e1243ba4ecb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.RNN_SURV at 0x7f3747079400>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rnnNet.build_model( batch_size = 30)"
      ],
      "metadata": {
        "id": "RjgN-3HQl1t5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44b4ded9-ba64-4c58-a525-97b22041ecd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.9/dist-packages/tensorflow/python/util/dispatch.py:1176: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "<ipython-input-5-d5b01bcb3511>:149: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
            "  cell = tf.nn.rnn_cell.LSTMCell(state_size, state_is_tuple=True)\n",
            "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-5-d5b01bcb3511>:214: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.9/dist-packages/keras/layers/rnn/legacy_cells.py:1048: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTM cell\n",
            "LSTM cell\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.RNN_SURV at 0x7f3747079400>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "rnnNet.train_RNN( \n",
        "              batch_size=30,\n",
        "              data=elData,\n",
        "              dataset='myData',\n",
        "              dropout_hid_state=0.1,\n",
        "              dropout_keep_prob_input=0.1,\n",
        "              dropout_keep_prob_output=0.1,\n",
        "              dropout_keep_prob_state=0.1,\n",
        "              num_epochs_per_time=5,\n",
        "              patience_max=3) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5q9HnD0mY30T",
        "outputId": "23ceda17-a547-44f6-e558-887dc0031f53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "path:  ./checkpoints/myData/state_size3.ckpt\n",
            "NO SAVED MODEL\n",
            "c-index_validation:  0.5045944866160608\n",
            "UPDATED\n",
            "c-index_validation:  0.44459981355706485\n",
            "c-index_validation:  0.47001376126426064\n",
            "c-index_validation:  0.5\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5045944866160608"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rnnNet.test(data=elData,\n",
        "         dataset='myData', \n",
        "         input_size=30,\n",
        "         is_training=False)"
      ],
      "metadata": {
        "id": "KXwy92Xg6OMe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b4a1c6dc-5eb3-4ffa-c97c-e0e4fc1f784d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-d5b01bcb3511>:149: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
            "  cell = tf.nn.rnn_cell.LSTMCell(state_size, state_is_tuple=True)\n",
            "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTM cell\n",
            "LSTM cell\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1377\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1378\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1379\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1360\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0m\u001b[1;32m   1362\u001b[0m                                       target_list, run_metadata)\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1453\u001b[0m                           run_metadata):\n\u001b[0;32m-> 1454\u001b[0;31m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[0m\u001b[1;32m   1455\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: ConcatOp : Dimension 0 in both shapes must be equal: shape[0] = [241,18] vs. shape[1] = [30,3]\n\t [[{{node rnn/while/rnn/multi_rnn_cell/cell_0/lstm_cell/concat}}]]",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-e90aa8460bb4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m rnnNet.test(data=elData,\n\u001b[0m\u001b[1;32m      2\u001b[0m          \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'myData'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m          \u001b[0minput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m          is_training=False)\n",
            "\u001b[0;32m<ipython-input-5-d5b01bcb3511>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self, data, dataset, input_size, is_training)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m             \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_X\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msurvivor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_hid_state\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_keep_prob_output\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_keep_prob_input\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_keep_prob_state\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m             \u001b[0mtrue_labels_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcox_output_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msurvivor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcox_output\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m             \u001b[0mC_index_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconcordance_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mcox_output_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    966\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 968\u001b[0;31m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0m\u001b[1;32m    969\u001b[0m                          run_metadata_ptr)\n\u001b[1;32m    970\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1189\u001b[0m     \u001b[0;31m# or if the call is a partial run that specifies feeds.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1191\u001b[0;31m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0m\u001b[1;32m   1192\u001b[0m                              feed_dict_tensor, options, run_metadata)\n\u001b[1;32m   1193\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1370\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1371\u001b[0;31m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0m\u001b[1;32m   1372\u001b[0m                            run_metadata)\n\u001b[1;32m   1373\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1395\u001b[0m                     \u001b[0;34m'\\nsession_config.graph_options.rewrite_options.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1396\u001b[0m                     'disable_meta_optimizer = True')\n\u001b[0;32m-> 1397\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=no-value-for-parameter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1399\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'rnn/while/rnn/multi_rnn_cell/cell_0/lstm_cell/concat' defined at (most recent call last):\n    File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n      app.start()\n    File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 612, in start\n      self.io_loop.start()\n    File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n      lambda f: self._run_callback(functools.partial(callback, future))\n    File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n      ret = callback()\n    File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n      self.ctx_run(self.run)\n    File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n      yielded = self.gen.send(value)\n    File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 365, in process_one\n      yield gen.maybe_future(dispatch(*args))\n    File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 268, in dispatch_shell\n      yield gen.maybe_future(handler(stream, idents, msg))\n    File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 543, in execute_request\n      self.do_execute(\n    File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 306, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2854, in run_cell\n      result = self._run_cell(\n    File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2881, in _run_cell\n      return runner(coro)\n    File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3057, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3249, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3326, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"<ipython-input-11-e90aa8460bb4>\", line 1, in <module>\n      rnnNet.test(data=elData,\n    File \"<ipython-input-5-d5b01bcb3511>\", line 426, in test\n      RNN_SURV.build_model(self, batch_size = input_size)\n    File \"<ipython-input-5-d5b01bcb3511>\", line 223, in build_model\n      rnn_outputs, final_state = execute_rnn(cell, X, init_state_fw)\n    File \"<ipython-input-5-d5b01bcb3511>\", line 214, in execute_rnn\n      rnn_outputs, final_state =  tf.nn.dynamic_rnn(cell, X_input, initial_state = init_state_fw, dtype=tf.float32)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/layers/rnn/legacy_cells.py\", line 213, in __call__\n      return super().__call__(inputs, state)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/legacy_tf_layers/base.py\", line 622, in __call__\n      outputs = super().__call__(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/base_layer_v1.py\", line 838, in __call__\n      outputs = call_fn(cast_inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/layers/rnn/legacy_cells.py\", line 1324, in call\n      cur_inp, new_state = cell(cur_inp, cur_state)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/layers/rnn/legacy_cell_wrappers.py\", line 145, in __call__\n      return self._call_wrapped_cell(\n    File \"/usr/local/lib/python3.9/dist-packages/keras/layers/rnn/legacy_cell_wrappers.py\", line 465, in _call_wrapped_cell\n      output, new_state = cell_call_fn(inputs, state, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/layers/rnn/legacy_cells.py\", line 379, in __call__\n      return base_layer.Layer.__call__(\n    File \"/usr/local/lib/python3.9/dist-packages/keras/legacy_tf_layers/base.py\", line 622, in __call__\n      outputs = super().__call__(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/base_layer_v1.py\", line 838, in __call__\n      outputs = call_fn(cast_inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/layers/rnn/legacy_cells.py\", line 1131, in call\n      lstm_matrix = tf.matmul(tf.concat([inputs, m_prev], 1), self._kernel)\nNode: 'rnn/while/rnn/multi_rnn_cell/cell_0/lstm_cell/concat'\nConcatOp : Dimension 0 in both shapes must be equal: shape[0] = [241,18] vs. shape[1] = [30,3]\n\t [[{{node rnn/while/rnn/multi_rnn_cell/cell_0/lstm_cell/concat}}]]\n\nOriginal stack trace for 'rnn/while/rnn/multi_rnn_cell/cell_0/lstm_cell/concat':\n  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 612, in start\n    self.io_loop.start()\n  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n    ret = callback()\n  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n    self.ctx_run(self.run)\n  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n    yielded = self.gen.send(value)\n  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 365, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n    yielded = ctx_run(next, result)\n  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 268, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n    yielded = ctx_run(next, result)\n  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 543, in execute_request\n    self.do_execute(\n  File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n    yielded = ctx_run(next, result)\n  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 306, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2854, in run_cell\n    result = self._run_cell(\n  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2881, in _run_cell\n    return runner(coro)\n  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3057, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3249, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3326, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-11-e90aa8460bb4>\", line 1, in <module>\n    rnnNet.test(data=elData,\n  File \"<ipython-input-5-d5b01bcb3511>\", line 426, in test\n    RNN_SURV.build_model(self, batch_size = input_size)\n  File \"<ipython-input-5-d5b01bcb3511>\", line 223, in build_model\n    rnn_outputs, final_state = execute_rnn(cell, X, init_state_fw)\n  File \"<ipython-input-5-d5b01bcb3511>\", line 214, in execute_rnn\n    rnn_outputs, final_state =  tf.nn.dynamic_rnn(cell, X_input, initial_state = init_state_fw, dtype=tf.float32)\n  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/util/deprecation.py\", line 357, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/util/traceback_utils.py\", line 150, in error_handler\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/util/dispatch.py\", line 1176, in op_dispatch_handler\n    return dispatch_target(*args, **kwargs)\n  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/ops/rnn.py\", line 746, in dynamic_rnn\n    (outputs, final_state) = _dynamic_rnn_loop(\n  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/ops/rnn.py\", line 952, in _dynamic_rnn_loop\n    _, output_final_ta, final_state = control_flow_ops.while_loop(\n  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 2795, in while_loop\n    result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants,\n  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 2279, in BuildLoop\n    original_body_result, exit_vars = self._BuildLoop(\n  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 2198, in _BuildLoop\n    body_result = body(*packed_vars_for_body)\n  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 2756, in <lambda>\n    body = lambda i, lv: (i + 1, orig_body(*lv))\n  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/ops/rnn.py\", line 929, in _time_step\n    (output, new_state) = call_cell()\n  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/ops/rnn.py\", line 915, in <lambda>\n    call_cell = lambda: cell(input_t, state)\n  File \"/usr/local/lib/python3.9/dist-packages/keras/layers/rnn/legacy_cells.py\", line 213, in __call__\n    return super().__call__(inputs, state)\n  File \"/usr/local/lib/python3.9/dist-packages/keras/legacy_tf_layers/base.py\", line 622, in __call__\n    outputs = super().__call__(inputs, *args, **kwargs)\n  File \"/usr/local/lib/python3.9/dist-packages/keras/engine/base_layer_v1.py\", line 838, in __call__\n    outputs = call_fn(cast_inputs, *args, **kwargs)\n  File \"/usr/local/lib/python3.9/dist-packages/keras/layers/rnn/legacy_cells.py\", line 1324, in call\n    cur_inp, new_state = cell(cur_inp, cur_state)\n  File \"/usr/local/lib/python3.9/dist-packages/keras/layers/rnn/legacy_cell_wrappers.py\", line 145, in __call__\n    return self._call_wrapped_cell(\n  File \"/usr/local/lib/python3.9/dist-packages/keras/layers/rnn/legacy_cell_wrappers.py\", line 465, in _call_wrapped_cell\n    output, new_state = cell_call_fn(inputs, state, **kwargs)\n  File \"/usr/local/lib/python3.9/dist-packages/keras/layers/rnn/legacy_cells.py\", line 379, in __call__\n    return base_layer.Layer.__call__(\n  File \"/usr/local/lib/python3.9/dist-packages/keras/legacy_tf_layers/base.py\", line 622, in __call__\n    outputs = super().__call__(inputs, *args, **kwargs)\n  File \"/usr/local/lib/python3.9/dist-packages/keras/engine/base_layer_v1.py\", line 838, in __call__\n    outputs = call_fn(cast_inputs, *args, **kwargs)\n  File \"/usr/local/lib/python3.9/dist-packages/keras/layers/rnn/legacy_cells.py\", line 1131, in call\n    lstm_matrix = tf.matmul(tf.concat([inputs, m_prev], 1), self._kernel)\n  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/util/traceback_utils.py\", line 150, in error_handler\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/util/dispatch.py\", line 1176, in op_dispatch_handler\n    return dispatch_target(*args, **kwargs)\n  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/ops/array_ops.py\", line 1821, in concat\n    return gen_array_ops.concat_v2(values=values, axis=axis, name=name)\n  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 1276, in concat_v2\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/framework/op_def_library.py\", line 795, in _apply_op_helper\n    op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/framework/ops.py\", line 3798, in _create_op_internal\n    ret = Operation(\n"
          ]
        }
      ]
    }
  ]
}